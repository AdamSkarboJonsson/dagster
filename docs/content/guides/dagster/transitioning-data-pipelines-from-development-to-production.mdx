---
title: "Transitioning data pipelines from development to production | Dagster Docs"
description: This guide walks through how to transition your data pipelines from local development to production
---

# Transitioning data pipelines from development to production

In this guide, we'll walk through how to transition your data pipelines from local development to staging and production deployments.

Let's say we’ve been tasked with fetching the `N` most recent entries from Hacker News and splitting it into two datasets: one containing all of the data about stories and one containing all of the data about comments. In order to make the pipeline maintainable and testable, we have two additional requirements:

- We must be able to run our data pipeline in local, staging, and production environments.
- We need to be confident that data won't be accidentally overwritten (for example because a user forgot to change a configuration value).

Using a few Dagster concepts, we can easily tackle this task! Here’s an overview of the main concepts we’ll be using in this guide:

| Concept                                             | Description                                                                                                                                                                                                                                                                          |
| --------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [Assets](/concepts/assets/software-defined-assets)  | An asset is a software object that models a data asset. For example, a table in a database or a file in cloud storage.                                                                                                                                                               |
| [Resources](/concepts/resources)                    | A resource is an object that typically models a connection to a external service. Resources can be shared between assets and different implementations of resources can be used depending on the environment. For example, a resource may provide methods to send messages in Slack. |
| [Run config](/concepts/configuration/config-schema) | Assets and resources sometimes require configuration to set certain values, like the password to a database. Run config allows you to set these values at run time. In this guide, we'll also use an API to set some default run configuration.                                      |

Using these Dagster concepts we will:

- Write three assets, including the full Hacker News dataset, data about comments, and data about stories
- Use Dagster's Snowflake resource (<PyObject object="SnowflakeResource" module="dagster_snowflake" />) to handle the connection to Snowflake[Snowflake](https://www.snowflake.com/)
- Set up our Dagster code so that the configuration for the Snowflake resource is automatically supplied based on the environment where the code is running

---

## Setup

<CodeReferenceLink filePath="examples/development_to_production/" />

To follow along with this guide, run the following to copy the full code example and install a few additional pip libraries:

```bash
dagster project from-example --name my-dagster-project --example development_to_production
cd my-dagster-project
pip install -e .
```

---

## Part 1: Local development

In this section, we'll:

- Write the assets
- Add run configuration for the Snowflake resource
- Materialize assets in the Dagster UI

### Step 1: Write the assets

Let’s start by writing the assets. We'll use Pandas DataFrames to interact with the data.

```python file=/guides/dagster/development_to_production/assets.py startafter=start_assets endbefore=end_assets
# assets.py
import pandas as pd
import requests
from dagster_snowflake import SnowflakeResource
from snowflake.connector.pandas_tools import write_pandas

from dagster import Config, asset


class ItemsConfig(Config):
    base_item_id: int


CREATE_UPDATE_TABLE_QUERY = """
create table if not exists {table_name} (
    id varchar,
    parent varchar,
    time datetime,
    type varchar,
    user_id varchar,
    text varchar,
    kids varchar,
    score double,
    title varchar,
    descendants varchar,
    url varchar
);

delete from {table_name} where id = id;

insert into {table_name}
select
    id,
    parent,
    time,
    type,
    user_id,
    text,
    kids,
    score,
    title,
    descendants,
    url
from items
where type = '{item_type}'
"""


@asset
def items(config: ItemsConfig, snowflake_resource: SnowflakeResource):
    """Items from the Hacker News API: each is a story or a comment on a story."""
    rows = []
    max_id = requests.get(
        "https://hacker-news.firebaseio.com/v0/maxitem.json", timeout=5
    ).json()
    # Hacker News API is 1-indexed, so adjust range by 1
    for item_id in range(max_id - config.base_item_id + 1, max_id + 1):
        item_url = f"https://hacker-news.firebaseio.com/v0/item/{item_id}.json"
        rows.append(requests.get(item_url, timeout=5).json())

    # ITEM_FIELD_NAMES is a list of the column names in the Hacker News dataset
    result = pd.DataFrame(rows, columns=ITEM_FIELD_NAMES).drop_duplicates(subset=["id"])
    result.rename(columns={"by": "user_id"}, inplace=True)

    # Upload data to Snowflake as a dataframe
    with snowflake_resource.get_connection() as conn:
        write_pandas(
            conn=conn,
            df=result,
            table_name="items",
            database=snowflake_resource.database,
            auto_create_table=True,
            use_logical_type=True,
        )


@asset(deps=[items])
def comments(snowflake_resource: SnowflakeResource):
    """Comments from the Hacker News API."""
    table_name = "comments"
    item_type = "comment"

    create_table = CREATE_UPDATE_TABLE_QUERY.format(
        table_name=table_name,
        item_type=item_type,
    )

    with snowflake_resource.get_connection() as conn:
        conn.cursor.execute(create_table)


@asset(deps=[items])
def stories(snowflake_resource: SnowflakeResource):
    """Stories from the Hacker News API."""
    table_name = "stories"
    item_type = "story"

    create_table = CREATE_UPDATE_TABLE_QUERY.format(
        table_name=table_name,
        item_type=item_type,
    )

    with snowflake_resource.get_connection() as conn:
        conn.cursor.execute(create_table)
```

### Step 2: Add run configuration

Next, we'll add the assets to our project's <PyObject object="Definitions" /> object and materialize them via the UI. We'll pass our credentials to the <PyObject object="SnowflakeResource" module="dagster_snowflake" />.

```python file=/guides/dagster/development_to_production/repository/repository_v1.py startafter=start endbefore=end
# __init__.py
from dagster_snowflake import SnowflakeResource

from dagster import Definitions
from development_to_production.assets import comments, items, stories

# Note that storing passwords in configuration is bad practice. It will be resolved later in the guide.
resources = {
    "snowflake_resource": SnowflakeResource(
        account="abc1234.us-east-1",
        user="me@company.com",
        # password in config is bad practice
        password="my_super_secret_password",
        database="LOCAL",
        schema="ALICE",
    ),
}

defs = Definitions(assets=[items, comments, stories], resources=resources)
```

**Note**: We've added our password to the configuration, which is bad practice. We'll resolve it shortly.

When we start the UI, we'll see an asset graph that looks like this:

<Image
alt="alt"
src="/images/guides/development_to_production/hacker_news_asset_graph.png"
width={1538}
height={730}
/>

### Step 3: Materialize the assets

After we've materialized the assets, let's verify that the data appeared correctly in Snowflake:

<Image
alt="alt"
src="/images/guides/development_to_production/snowflake_data.png"
width={2304}
height={1064}
/>

When we materialized the assets, Dagster used the <PyObject object="SnowflakeResource" module="dagster_snowflake" /> to connect to Snowflake. Once connected, the SQL queries in contained in the function body of each asset was executed to create the `ITEMS`, `COMMENTS`, and `STORIES` tables.

---

## Part 2: Deployment

In this section, we'll:

- Modify the configuration for the Snowflake resource to handle staging and production environments
- Discuss different options for managing a staging environment

Now that our assets work locally, we can start the deployment process! We'll first set up our assets for production and then discuss the options for our staging deployment.

### Step 1: Configure production

We want to store the assets in a production Snowflake database, so we need to update the configuration for the <PyObject object="SnowflakeResource" module="dagster_snowflake" />.

However, if we were to only update the values we set for local development, we'd run into an issue: the next time someone wants to work on these assets, they'll need to remember to change the configuration back to the local values. This means a production asset could be accidentally overwritten during local development.

#### Step 1.1: Create environment-dependent configuration

Instead, let's set up the configuration for the <PyObject object="SnowflakeResource" module="dagster_snowflake" /> to depend on the current environment:

```python file=/guides/dagster/development_to_production/repository/repository_v2.py startafter=start endbefore=end
# __init__.py

# Note that storing passwords in configuration is bad practice. It will be resolved soon.
resources = {
    "local": {
        "snowflake_resource": SnowflakeResource(
            account="abc1234.us-east-1",
            user="me@company.com",
            # password in config is bad practice
            password="my_super_secret_password",
            database="LOCAL",
            schema="ALICE",
        ),
    },
    "production": {
        "snowflake_resource": SnowflakeResource(
            account="abc1234.us-east-1",
            user="dev@company.com",
            # password in config is bad practice
            password="company_super_secret_password",
            database="PRODUCTION",
            schema="HACKER_NEWS",
        ),
    },
}
deployment_name = os.getenv("DAGSTER_DEPLOYMENT", "local")

defs = Definitions(
    assets=[items, comments, stories], resources=resources[deployment_name]
)
```

Don't worry about the passwords still in the configuration - we'll fix that in the next step.

Now, we can set the environment variable `DAGSTER_DEPLOYMENT=production` in our deployment and the correct resources will be applied to the assets!

#### Step 1.2: Create environment variables

Even though we now have environment-dependent configuration, there are still a few issues with our setup:

1. Developers need to remember to change the `user`, `password`, and `schema` values when developing locally
2. Passwords are being stored in code

Using environment variables, we can easily solve both of these problems. <PyObject object="EnvVar" /> allows us to source configuration for resources from environment variables.

Now, we can store the Snowflake configuration values as environment variables and point the resource to them:

```python file=/guides/dagster/development_to_production/repository/repository_v3.py startafter=start endbefore=end
# __init__.py


resources = {
    "local": {
        "snowflake_resource": SnowflakeResource(
            account="abc1234.us-east-1",
            user=EnvVar("DEV_SNOWFLAKE_USER"),
            password=EnvVar("DEV_SNOWFLAKE_PASSWORD"),
            database="LOCAL",
            schema=EnvVar("DEV_SNOWFLAKE_SCHEMA"),
        ),
    },
    "production": {
        "snowflake_resource": SnowflakeResource(
            account="abc1234.us-east-1",
            user="system@company.com",
            password=EnvVar("SYSTEM_SNOWFLAKE_PASSWORD"),
            database="PRODUCTION",
            schema="HACKER_NEWS",
        ),
    },
}
deployment_name = os.getenv("DAGSTER_DEPLOYMENT", "local")

defs = Definitions(
    assets=[items, comments, stories], resources=resources[deployment_name]
)
```

### Step 2: Configure staging

Depending on your organization’s Dagster setup, there are a couple of options for a staging environment:

- **For Dagster Cloud users**, we recommend using [Branch Deployments](/dagster-cloud/managing-deployments/branch-deployments) as your staging step. A Branch Deployment is a new Dagster deployment that is automatically generated for each git branch. Check out the [Branch Deployment documentation](/guides/dagster/branch_deployments) for more info.

- **For a self-hosted staging deployment**, we’ve already done most of the necessary work to run our assets in staging! All we need to do is add another entry to the `resources` dictionary and set `DAGSTER_DEPLOYMENT=staging` in our staging deployment.

In the following example, we added an entry for `staging` to the `resources` dictionary:

```python file=/guides/dagster/development_to_production/repository/repository_v3.py startafter=start_staging endbefore=end_staging
resources = {
    "local": {...},
    "production": {...},
    "staging": {
        "snowflake_resource": SnowflakeResource(
            account="abc1234.us-east-1",
            user="system@company.com",
            password=EnvVar("SYSTEM_SNOWFLAKE_PASSWORD"),
            database="STAGING",
            schema="HACKER_NEWS",
        ),
    },
}
```

---

## Advanced: Unit tests with stubs and mocks

You may have noticed a missing step in the development workflow presented in this guide — unit tests! While the main purpose of this guide is to help you transition your code from local to production deployment, unit testing is still an important part of the development cycle. In this section, we'll explore a pattern you may find useful when writing your own unit tests.

When writing unit tests for the `items` asset, we could make more precise assertions if we knew exactly what data we'd receive from Hacker News. If we refactor our interactions with the Hacker News API as a resource, we can leverage Dagster's resource system to provide a stub resource in our unit tests.

Before we get into the implementation, let's go over some best practices:

- [When to use resources](#when-to-use-resources)
- [When to use stub resources](#when-to-use-stub-resources)

### When to use resources

In many cases, interacting with an external service directly in assets or ops is more convenient than refactoring the interactions with the service as a resource.

We recommend refactoring code to use resources in the following cases:

- Multiple assets or ops need to interact with the service in a consistent way
- Different implementations of a service need to be used in certain scenarios (ie. a staging environment, or unit tests)

### When to use stub resources

Determining when it makes sense to stub a resource for a unit test can be a topic of much debate. In some cases, it would be too complicated to write and maintain a stub. For example, mocking a database like Snowflake with a lightweight database could be difficult because the SQL syntax and runtime behavior may vary.

In general, if a resource is relatively simple, writing a stub can be helpful for unit testing the assets and ops that use the resource.

We'll start by writing the "real" Hacker News API Client:

```python file=/guides/dagster/development_to_production/resources/resources_v1.py startafter=start_resource endbefore=end_resource
# resources.py
from typing import Any, Dict, Optional

import requests

from dagster import ConfigurableResource


class HNAPIClient(ConfigurableResource):
    """Hacker News client that fetches live data."""

    def fetch_item_by_id(self, item_id: int) -> Optional[Dict[str, Any]]:
        """Fetches a single item from the Hacker News API by item id."""
        item_url = f"https://hacker-news.firebaseio.com/v0/item/{item_id}.json"
        item = requests.get(item_url, timeout=5).json()
        return item

    def fetch_max_item_id(self) -> int:
        return requests.get(
            "https://hacker-news.firebaseio.com/v0/maxitem.json", timeout=5
        ).json()

    @property
    def item_field_names(self) -> list:
        # omitted for brevity, see full code example for implementation
        return []
```

Now we'll update the `items` asset to use this client as a resource. **Note**: For the sake of brevity, we've omitted the implementation of the property `item_field_names` in `HNAPIClient`. You can find the full implementation of this resource in the [full code example](https://github.com/dagster-io/dagster/tree/master/examples/development_to_production) on GitHub.

```python file=/guides/dagster/development_to_production/assets_v2.py startafter=start_items endbefore=end_items
# assets.py


class ItemsConfig(Config):
    base_item_id: int


@asset
def items(
    config: ItemsConfig, snowflake_resource: SnowflakeResource, hn_client: HNAPIClient
):
    """Items from the Hacker News API: each is a story or a comment on a story."""
    rows = []
    max_id = hn_client.fetch_max_item_id()

    # Hacker News API is 1-indexed, so adjust range by 1
    for item_id in range(max_id - config.base_item_id + 1, max_id + 1):
        rows.append(hn_client.fetch_item_by_id(item_id))

    result = pd.DataFrame(rows, columns=hn_client.item_field_names).drop_duplicates(
        subset=["id"]
    )
    result.rename(columns={"by": "user_id"}, inplace=True)

    # Upload data to Snowflake as a dataframe
    with snowflake_resource.get_connection() as conn:
        write_pandas(
            conn=conn,
            df=result,
            table_name="items",
            database=snowflake_resource.database,
            auto_create_table=True,
            use_logical_type=True,
        )
```

Next, we'll add an instance of `HNAPIClient` to `resources` to our project's `Definitions` object:

```python file=/guides/dagster/development_to_production/repository/repository_v3.py startafter=start_hn_resource endbefore=end_hn_resource
resource_defs = {
    "local": {"hn_client": HNAPIClient(), "snowflake_resource": {...}},
    "production": {"hn_client": HNAPIClient(), "snowflake_resource": {...}},
    "staging": {"hn_client": HNAPIClient(), "snowflake_resource": {...}},
}
```

Now we can write a stubbed version of the Hacker News resource. We want to make sure the stub has implementations for each method `HNAPIClient` implements.

<Note>
  <strong>Heads up!</strong> Since the stub Hacker News resource and the real
  Hacker News resource need to implement the same methods, this would be a great
  time to write an interface. We’ll skip the implementation in this guide, but
  you can find it in the{" "}
  <a href="https://github.com/dagster-io/dagster/tree/master/examples/development_to_production">
    full code example
  </a>
  .
</Note>

```python file=/guides/dagster/development_to_production/resources/resources_v2.py startafter=start_mock endbefore=end_mock
# resources.py


class StubHNClient:
    """Hacker News Client that returns fake data."""

    def __init__(self):
        self.data = {
            1: {
                "id": 1,
                "type": "comment",
                "title": "the first comment",
                "by": "user1",
            },
            2: {"id": 2, "type": "story", "title": "an awesome story", "by": "user2"},
        }

    def fetch_item_by_id(self, item_id: int) -> Optional[Dict[str, Any]]:
        return self.data.get(item_id)

    def fetch_max_item_id(self) -> int:
        return 2

    @property
    def item_field_names(self) -> list:
        return ["id", "type", "title", "by"]
```

Now we can use the stub Hacker News resource to test that the `items` asset transforms the data in the way we expect:

```python file=/guides/dagster/development_to_production/test_assets.py startafter=start endbefore=end
# test_assets.py


def test_items():
    hn_dataset = items(
        config=ItemsConfig(base_item_id=StubHNClient().fetch_max_item_id()),
        hn_client=StubHNClient(),
    )
    assert isinstance(hn_dataset, pd.DataFrame)

    expected_data = pd.DataFrame(StubHNClient().data.values()).rename(
        columns={"by": "user_id"}
    )

    assert (hn_dataset == expected_data).all().all()
```

---

## Conclusion

This guide demonstrates how we recommend writing your assets and jobs so that they transition from local development to staging and production environments without requiring code changes at each step. While we focused on assets in this guide, the same concepts and APIs can be used to swap out run configuration for jobs.
